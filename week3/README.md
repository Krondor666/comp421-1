This week is about learning the parameters of a predictor. We'll look at three flavours: polynomials, neural nets, and radial basis functions. We will see that each has its own strengths and weaknesses.

***
# fitting polynomials to data
< 1st lecture (presented by James) >

Today's lecture is a tour of the [first section]() of Chris Bishop's fantastic book [Pattern Recognition and Machine Learning](http://research.microsoft.com/en-us/um/people/cmbishop/prml/). The whole book is well worth getting if you plan to go further with machine learning - see the library for the hardcopy.

 
***

# Back-propagation in neural networks
< 2nd lecture (presented by Jack) >



***

# Radial basis function networks
< 3rd lecture (presented by ??? and ???) >

